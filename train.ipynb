{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4a47e4a-21dd-474b-a735-f72d29931e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from environment import Env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cde7ae5b-b2aa-470f-a76c-338430ae0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccdcdfb7-c756-4db3-aba4-a80682a46295",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=5, stride=2, padding=1)\n",
    "        # self.conv3 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.linear = nn.Linear(288, 225)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out2 = self.conv2(out1)\n",
    "        out3 = self.flat(out2)\n",
    "        # out4 = self.softmax(self.linear(out3))\n",
    "        out4 = self.linear(out3)\n",
    "        return out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ea7e9cd-955c-4a6f-bdf3-e4e3cad9a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_move(player, model_eval, model_target, game, device):\n",
    "    chess_map, not_pos, _, _ = game.observe()\n",
    "    situ = torch.Tensor(chess_map).to(device)\n",
    "    situ = torch.reshape(situ, (1, 2, 15, 15))\n",
    "    # print(situ.shape)\n",
    "    out_eval = model_eval(situ)\n",
    "    out_target = model_target(situ)\n",
    "    # print(out_eval)\n",
    "    out_target = out_target.cpu().detach().numpy().reshape((225,))\n",
    "    out_eval = out_eval.cpu().detach().numpy()\n",
    "    move_matrix = out_eval.reshape((225,))\n",
    "    # 转换为一维数组下标\n",
    "    not_pos = not_pos[:, 0] * 15 + not_pos[:, 1]\n",
    "    # 删除不可落子的位置\n",
    "    # print(move_matrix.shape)\n",
    "    move_matrix[not_pos] = move_matrix[not_pos] - 99\n",
    "    out_target[not_pos] = out_target[not_pos] * 0\n",
    "    max_pos = int(np.argmax(move_matrix))\n",
    "    # print(move_matrix.shape)\n",
    "    # print(max_pos)\n",
    "    move_pos = np.unravel_index(max_pos, chess_map[0, :, :].shape)\n",
    "    # 进行落子\n",
    "    state = game.move(player, move_pos[0], move_pos[1])\n",
    "    return chess_map, state, out_target, move_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d29f9c9d-e10d-4480-826f-785d68d9edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(epoch):\n",
    "    eval_0 = net().to(device)\n",
    "    eval_1 = net().to(device)\n",
    "    target_0 = net().to(device)\n",
    "    target_1 = net().to(device)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer_0 = optim.Adam(eval_0.parameters(), lr=0.001)\n",
    "    optimizer_1 = optim.Adam(eval_1.parameters(), lr=0.001)\n",
    "    for i in range(epoch):\n",
    "        if i % 5 == 0:\n",
    "            target_0 = eval_0\n",
    "            target_1 = eval_1\n",
    "        # 进行游戏\n",
    "        memory_situation_0 = []\n",
    "        memory_target_0 = []\n",
    "        memory_situation_1 = []\n",
    "        memory_target_1 = []\n",
    "        move_memory = pd.DataFrame(columns=['player', 'move_x', 'move_y'])\n",
    "        game = Env()\n",
    "        for _ in range(225):\n",
    "            # 白棋落子\n",
    "            chess_map, state, target, move_pos_0 = player_move(0, eval_0, target_0, game, device)\n",
    "            move_memory.loc[len(move_memory)] = [0, move_pos_0[0], move_pos_0[1]]\n",
    "            target *= ALPHA\n",
    "            memory_situation_0.append(chess_map.tolist())\n",
    "            memory_target_0.append(target.tolist())\n",
    "            if state == True:\n",
    "                move_pos_0 = move_pos_0[0] * 15 + move_pos_0[1]\n",
    "                move_pos_1 = move_pos_1[0] * 15 + move_pos_1[1]\n",
    "                # 奖励自己最后一步\n",
    "                memory_target_0[-1][int(move_pos_0)] += 1\n",
    "                # 惩罚对手最后一步\n",
    "                memory_target_1[-1][int(move_pos_1)] += -0.9\n",
    "                break\n",
    "            # 黑棋落子\n",
    "            chess_map, state, target, move_pos_1 = player_move(1, eval_1, target_1, game, device)\n",
    "            move_memory.loc[len(move_memory)] = [1, move_pos_1[0], move_pos_1[1]]\n",
    "            target *= ALPHA\n",
    "            memory_situation_1.append(chess_map.tolist())\n",
    "            memory_target_1.append(target.tolist())\n",
    "            if state == True:\n",
    "                move_pos_0 = move_pos_0[0] * 15 + move_pos_0[1]\n",
    "                move_pos_1 = move_pos_1[0] * 15 + move_pos_1[1]\n",
    "                # 奖励自己最后一步\n",
    "                memory_target_1[-1][int(move_pos_1)] += 1\n",
    "                # 惩罚对手最后一步\n",
    "                memory_target_0[-1][int(move_pos_0)] += -0.9\n",
    "                break\n",
    "        # print(\"game over!\")\n",
    "        move_memory.to_csv('move_dir/game_' + str(i) + '.csv', sep=',')\n",
    "        # 整合成dataloader\n",
    "        m_s_0 = np.array(memory_situation_0)\n",
    "        m_t_0 = np.array(memory_target_0)\n",
    "        m_s_1 = np.array(memory_situation_1)\n",
    "        m_t_1 = np.array(memory_target_1)\n",
    "        m_s_0 = torch.Tensor(m_s_0).to(torch.float32)\n",
    "        m_t_0 = torch.Tensor(m_t_0).to(torch.float32)\n",
    "        m_s_1 = torch.Tensor(m_s_1).to(torch.float32)\n",
    "        m_t_1 = torch.Tensor(m_t_1).to(torch.float32)\n",
    "        m_0 = TensorDataset(m_s_0, m_t_0)\n",
    "        m_1 = TensorDataset(m_s_1, m_t_1)\n",
    "        memoryLoader_0 = DataLoader(m_0, batch_size=8)\n",
    "        memoryLoader_1 = DataLoader(m_0, batch_size=8)\n",
    "        \n",
    "        # 训练\n",
    "        train(eval_0, memoryLoader_0, optimizer_0, criterion, device, 10, i)\n",
    "        train(eval_1, memoryLoader_1, optimizer_1, criterion, device, 10, i)\n",
    "        \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b19f4b20-6439-440a-8985-b8d4c2315220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_offense(epoch):\n",
    "    ALPHA = 0.9\n",
    "    eval_0 = net().to(device)\n",
    "    target_0 = net().to(device)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer_0 = optim.Adam(eval_0.parameters(), lr=0.001)\n",
    "    model_win = 0\n",
    "    for i in range(epoch):\n",
    "        black = AgentPlayer()\n",
    "        if i % 1 == 0:\n",
    "            target_0 = eval_0\n",
    "            print(model_win / epoch)\n",
    "        # 进行游戏\n",
    "        memory_situation_0 = []\n",
    "        memory_target_0 = []\n",
    "        move_memory = pd.DataFrame(columns=['player', 'move_x', 'move_y'])\n",
    "        game = Env()\n",
    "        for _ in range(225):\n",
    "            # 黑棋落子\n",
    "            m, _, _, p = game.observe()\n",
    "            black_pos = black.move_chess(chess_map=m, is_pos=p)\n",
    "            state = game.move(0, black_pos[0], black_pos[1])\n",
    "            move_memory.loc[len(move_memory)] = [0, black_pos[0], black_pos[1]]\n",
    "            if state == True:\n",
    "                move_pos_0 = move_pos_0[0] * 15 + move_pos_0[1]\n",
    "                # 惩罚对手最后一步\n",
    "                memory_target_0[-1][int(move_pos_0)] += -1\n",
    "                break\n",
    "            # 白棋落子\n",
    "            chess_map, state, target, move_pos_0 = player_move(1, eval_0, target_0, game, device)\n",
    "            move_memory.loc[len(move_memory)] = [1, move_pos_0[0], move_pos_0[1]]\n",
    "            target *= ALPHA\n",
    "            memory_situation_0.append(chess_map.tolist())\n",
    "            memory_target_0.append(target.tolist())\n",
    "            if state == True:\n",
    "                model_win += 1\n",
    "                move_pos_0 = move_pos_0[0] * 15 + move_pos_0[1]\n",
    "                # 奖励自己最后一步\n",
    "                memory_target_0[-1][int(move_pos_0)] += 1\n",
    "                break\n",
    "        # print(\"game over!\")\n",
    "        move_memory.to_csv('move_dir/game_' + str(i) + '.csv', sep=',')\n",
    "        # 整合成dataloader\n",
    "        m_s_0 = np.array(memory_situation_0)\n",
    "        m_t_0 = np.array(memory_target_0)\n",
    "        m_s_0 = torch.Tensor(m_s_0).to(torch.float32)\n",
    "        m_t_0 = torch.Tensor(m_t_0).to(torch.float32)\n",
    "        m_0 = TensorDataset(m_s_0, m_t_0)\n",
    "        memoryLoader_0 = DataLoader(m_0, batch_size=8)\n",
    "        # 训练\n",
    "        train(eval_0, memoryLoader_0, optimizer_0, criterion, device, 20, i)\n",
    "        \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9767126-48ab-447e-b1b2-7c9ce49848e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device, epoch, step):\n",
    "    game_log = pd.DataFrame(columns=['step', 'epoch', 'loss'])\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for _, data in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            situ, target = data[0].to(device), data[1].to(device)\n",
    "            output = model(situ)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss\n",
    "        # print(\"epoch %d: loss = %f\"%(i, train_loss))\n",
    "        game_log.loc[len(game_log)] = [step, i, train_loss.cpu().detach().numpy()]\n",
    "    game_log.to_csv('game_log.csv', sep=',', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a38b52-b8fa-41d7-984e-8a7b1e0dacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minimax_search import AgentPlayer\n",
    "learn_offense(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b925f5-1f57-471f-866b-a7eb960994f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e17691-6c8d-4930-882e-2356908108b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
